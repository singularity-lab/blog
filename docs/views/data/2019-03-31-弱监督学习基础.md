---
layout: post
title: 弱监督学习
date: 2019-04-04
author: iyin
categories:
  - 数据分析部
tags:
  - 机器学习算法
---

## 弱监督学习

参考：周志华教授的论文《A brief introduction to weakly supervised learning》、西瓜书

1. #### 要解决的问题

   由于数据标注过程的高成本，很多任务很难获得如全部真值标签这样的强监督信息。

   > 实际需求：要过滤虚假评论，需要确保分类器具有良好的泛化能力，获得大量的标注的标注语料是前提条件。一方面人工标注成本过高，另一方面只凭语义信息难以判定，标注准确率低，于是想到了半监督学习。
   >
   > 达观的文本挖掘系统就在多个模块里面都使用到了半监督学习的方法，主要方式是通过外部知识来对训练样本进行语义扩展，然后结合数量较多的未标注样本选取预测置信度高的子集作为新样本加入训练集进行模型训练。

2. #### 三种弱监督

   - 不完全监督（incomplete supervision）：只有一部分训练数据具备标签；
   - 不确切监督（inexact supervision）：例如图像训练数据只具备粗粒度标签；
   - 不准确监督（inaccurate supervision）：模型给出的标签并不总是真值，图片标注者不小心或比较疲倦，或者某些图片就是难以分类。

   ![graph1](/Blog-Share/img/1903/04/iyin/graph1.png)

   在实际操作中，几种弱监督经常同时发生。例如，在标注虚假评论数据时，人为标注的准确度平均只有百分之六十左右，是严重的不准确标注，并且人工标注 10 万条左右的数据是非常耗费精力的，1 万条左右不足以满足深度学习的需求，又属于不完全监督。即使是公开数据集，大多为平台根据自己模型过滤的结果，也存在不准确的情况。

   > 题外话：
   >
   > 一种设想：模仿一些研究中众包构建假评的模式，例如以电子问卷形式，给出商品信息，请用户编写虚假评论是否可行呢？
   >
   > 人工标注准确度过低，而众包模拟假评和实际水军的虚假评论也存在差别，另外，AI 水军的语义和结构与前面两者又不相同，与识别模型相爱相杀，最合理的标注方式似乎是先根据水军的判别结果进行标注，但根据用户对评论进行标注又存在数据过于稀疏的问题。

3. #### 不完全监督

   - 样本

     l 个标注样本 与 u = m-l 个未标注样本

   - 分类

     - **主动学习**：假设存在一个神谕，比如一位专家，能够像他查询选定的未标注事例的真值标签

     - 半监督学习：试图自动开发无标注数据以提高学习效果，不需要人工干预。

       **直推式学习**是一种特殊的半监督学习，它与**纯半监督学习**的区别主要在于，对于训练过的模型需要进行预测的数据，假设不同。

       > 直推式学习：封闭世界假设，测试数据是事先给出的，目标是优化测试数据的性能，即未标注数据正是测试数据。
       >
       > 纯半监督式学习：开放世界假设，测试数据是未知的，未标注数据不一定是测试数据。
       > ![graph2](/Blog-Share/img/1903/04/iyin/graph2.png)

   - 几种算法

     半监督学习要利用未标记样本，必然要做一些将未标记样本所揭示的数据分布信息与类别标记想联系的假设，其本质是“相似的样本拥有相似的输出”。

     基于这个假设，可以从不同的角度来使用非标记样本的这个特性，就形成了不同的半监督学习算法。

     - 生成式方法

       > 基于生成式模型，假设所有数据（无论是否有标记）都是由同一个潜在的模型生产的。这个假设使得我们能通过潜在模型的参数将未标注数据与学习目的关联起来，而未标注数据的标记则可看作模型的确实参数。
       > 通常可基于 EM 算法进行极大似然估计求解。在半监督学习生成式方法中，EM 算法使用逻辑如下：
       >
       > ［1］假设标注数据与未标注数据来源于同一潜在的分布模型，模型参数为 t
       >
       > ［2］给定参数的初始值 t0，我们可以根据分布模型计算出未标注数据的标注值 y0
       >
       > ［3］根据 y0 和标注数据使用极大似然估计估算此时参数 t 能使得数据概率最大的最优解 t1
       >
       > ［4］使用 t1 计算估算此时未标注数据的标注值 y1
       >
       > ［5］步骤［3］和步骤［4］不断迭代，直到收敛
       >
       > - 优点
       >
       >   简单，易于实现，在有标记数据极少的情形下往往比其他方法性能更好
       >
       > - 缺点
       >
       >   关键在于，假设的生成式模型必须与真实数据**分布吻合**，否则使用未标记数据反而会降低泛化性能。遗憾的是，在现实任务中往往很难事先做出准确的模型假设，除非拥有充分可靠的领域知识。

     - 半监督 SVM（Semi-supervised Support Vector Machine，简称 S3VM）

       > 相对于支持向量机要找出最大间隔划分超平面，S3VM 考虑了未标注样本的信息，试图找到能将两类有标记样本分开，且穿过数据低密度区域的划分超平面。如下图所示，这里的基本假设是**“低密度分隔”**（low-density separation）
       >
       > ![graph5](/Blog-Share/img/1903/04/iyin/graph5.png)
       >
       > 注：图标错了，黑色的是 SVM
       > 半监督支持向量机中最著名的是 TSVM（Transductive Support Vector Machine）。TSVM 试图考虑对未标记样本进行各种可能的标记指派，然后从中找出在所有标记和未标记样本上间隔最大化的划分超平面。
       >
       > 基本逻辑如下：
       >
       > ［1］先用标记好的样本训练一个 SVM 分类器
       >
       > ［2］用 SVM 对未标记数据的分类结果进行预测
       >
       > ［3］在预测的未标记数据分类中，找出指派的标记相反的且很可能错误的未标记样本对，将它们的标记对换，然后用现有的标记和未标记样本重新训练 SVM
       >
       > ［4］重复［2］、［3］直到到达停止条件
       >
       > ［5］至此在未标记数据的基础上获得了一个 SVM 分类器，即对已有的未标注样本进行了标记，还能用来对未见的样本进行预测
       >
       > - 解释：算法中提到的很可能错误的判断标准是具体样本的松弛向量；在使用样本时，标注样本和未标注样本的权重是不同的，一开始将未标注样本的权重设定为很小，每次迭代一次后将其权重调高，当未标注样本的权重高于标注样本权重时，到达停止条件。
       >
       > - 特点：搜寻标记指派可能出错的每一对未标记样本进行调整，是一个涉及巨大计算开销的大规模优化问题。因此，半监督 SVM 研究的一个重点是如何设计出高效的优化求解策略。

     - 图半监督学习

       > 将数据集映射成图，数据集中的每个样本对应于图中的一个结点，若两个样本之间的相似度或相关性很强，则对应的结点之间存在一条边，边的强度正比于样本之间的相似度。
       >
       > 将有标记的数据看作是被染色的节点，未标注的数据是未染色的节点。图半监督学习就对应于“颜色”在图上扩散和传播的过程。由于一个图对应了一个矩阵，这就使得我们能基于矩阵运算来进行半监督学习算法的推导与分析。
       > 基本逻辑：
       >
       > ［1］基于数据集构建一个图 G = (V, E)，V 代表结点集合，E 代表边集合
       >
       > ［2］根据数据集中结点的相似度计算出图的边集 E 的亲和矩阵 W；其中 wij 代表第 i 个结点和第 j 个结点之间的相似度，一般可基于高斯函数定义
       >
       > ［3］假定从图 G 上可学得一个实值预测函数 f：V -> R，于是可定义关于 f 的“能量函数”，能量函数可理解为任意两个结点的预测结果的差值乘以它们之间的权重 wij 的和。很明显，能量函数最小化时即得到最优结果
       >
       > ［4］对能量函数求导，可得到未标注数据的预测结果
       >
       > - 优点：概念清晰，且易于通过对矩阵运算的分析来探索算法性质
       >
       > - 缺点：存储开销大，很难直接处理大规模数据；接收新样本时可能需要从新计算

     - 基于分歧的方法（disagreement-based methods）

       > 与前边介绍的只基于单学习器的方法不同，基于分歧的方法使用多个学习器来训练数据，并利用学习器之间的“分歧”对未标记样本进行利用。
       >
       > **多视图**：在不少现实应用中，一个数据对象往往同时拥有多个“属性集”（attribute set），每个属性集就构成了一个“视图”。
       >
       > **多视图的特性**：假设不同视图具有“相容性”（compatibility），即不同视图所包含的关于输出空间的信息是一致的：用电影的画面视图和声音视图来进行预测的输出都应该是判断电影类型，而不是一个是判断电影类型，一个是判断电影好评度。在此假设下，不同视图信息的“互补性”会给学习器的构建带来很多的便利
       >
       > **协同训练**：协同训练很好地利用了多视图的“相融互补性”。假设数据拥有两个充分（充分指每个视图都包含足以产生最优学习器的信息）且条件独立视图，则协同训练学习的基本逻辑如下：
       >
       > ［1］在每个视图上基于有标记样本分别训练出一个分类器
       >
       > ［2］让每个分类器分别挑选自己“最有把握的”未标记样本赋予伪标记
       >
       > ［3］将伪标记样本提供给另一个分类器作为新增的有标记样本用于训练更新
       >
       > ［4］重复［2］、［3］直到两个学习器不再变化或达到预设的迭代次数时停止
       >
       > - 优点：过程简单，并且若两个视图充分且条件独立，可利用未标记样本将弱分类器的泛化性能提升到任意高
       >
       > - 缺点：视图的条件独立性在现实任务中通常很难满足，因此性能提升幅度不会那么大

     - 半监督聚类（semi-supervised clustering）

       > 半监督聚类通过利用现实聚类任务中的额外监督信息来获得更好的聚类效果。（前边的算法是在监督学习的基础上引入未标注数据来优化性能，半监督聚类则是在非监督学习的基础上引入监督信息来优化性能，这同样也是另一个角度的“**半**”监督学习）
       >
       > 聚类任务中获得的监督信息大致有两种类型。第一种是“必连”（must-link）与“勿连”（cannot-link）约束，前者是指样本必属于同一簇，后者是指样本必不属于同一簇；第二种类型的监督信息则是少量的有标注样本。
       >
       > 1. **约束 k 均值（Constrained k-means）算法**：约束 k 均值算法是利用第一类监督信息的代表，其在 k 均值算法的基础上考虑了“必连”和“勿连”约束。
       >
       >    [1] 初始化 k 个簇，随机选择 k 个样本作为 k 个簇的初始均值向量
       >
       >    不断迭代以下几步：
       >
       >    [2] 对每个样本 x ，计算其与每个簇均值向量的距离 d，
       >
       >    [3] 找出距离样本 x 最近的簇 i ，判断将 x 放进 i 是否违反“约束”
       >
       >    \>> 如果不违反，则将簇 i 作为 x 的簇标记，并将 x 放入该簇集合 Ci 中
       >
       >    \>> 如果违反，则找次近的簇，以此类推直到找到满足约束的最近簇 i'，将 x 放入该簇集合 Ci' 中
       >
       >    [4] 对所有的簇集合，根据本次迭代得到的簇集合计算新的均值向量
       >
       >    [5] 当均值向量均未更新时，退出迭代步骤
       >
       >    最终获得聚类计算的结果簇划分
       >
       > 2. **约束种子 k 均值（Constrained Seed k-means）算法**：约束种子 k 均值算法使用少量标记数据来辅助聚类计算。与 k 均值算法一样，不同点在于用标记样本作为“种子”来初始化 k 均值算法的 k 个聚类中心，并且在聚类簇迭代更新过程中不改变种子样本的簇隶属关系。

4. #### 不确切监督

   虽然有监督信息，但是信息不够精确。

   一个典型的场景是仅有粗粒度的标签信息可用。

   ![graph3](/Blog-Share/img/1903/04/iyin/graph3.png)

   该方法为**多示例学习**

   - 几乎所有的有监督学习算法都有对等的多示例算法，大多数算法试图调整单示例监督学习算法，使其适配多示例表示，主要是将其关注点从对示例的识别转移到对包的识别；一些其他算法试图通过表示变换，调整多示例表示使其适配单示例算法。

     还有一种类型，将算法分为三类：一个整合了示例级响应的示例空间范式，一个把包视作一个整体的包空间范式，以及一个在嵌入特征空间中进行学习的嵌入空间范式中。

     > 这些示例通常被视为 i.i.d. 样本，然而，有研究表明，多示例学习中的示例不应该被认为是独立的，尽管这些包可以被视为 i.i.d. 样本，并且已经有一些有效的算法是基于此见解进行开发的。

   - 将真实对象（例如一幅图像或一个文本文档）视为一个包是很自然的。然而，有的时候需要为每个包生成示例。包生成器制定如何生成示例来构成包。

     通常情况下，可以从图像中提取许多小的图像块作为其示例，而章节/段落甚至句子可以用作文本文档的示例。包生成器对学习效果有重要影响，有关于图像包生成器的研究表明，一些简单的密集取样包生成器比一些复杂的生成器性能更好。

   - 几种假设：

     > 多示例学习的初始目标是未没有见过的包预测标签，但是识别之所以让正包变正的 key instance 对于没有细粒度标记训练数据的感兴趣的区域定位任务中非常有用。
     >
     > - 标准多示例学习的假设：每一个正包必须包含一个关键示例
     > - 假定不存在关键示例，每一个示例都对包标签有贡献
     > - 假定存在多个概念，仅当一个包包含满足所有概念的示例时，包才是正的
     >
     > 多示例学习对于包中每个示例都有不同规则分类的 heterogeneous 案例来说很难，对与相同规则分类的所有示例的同质性 homogeneous 案例可学习，而几乎所有的多实例任务都属于同质性案例。某些在包中任意分布的示例是可学习的，但分析难度太大了。

5. #### 不准确监督

   - 典型场景：有标签噪声

   - 基本思想：

     识别潜在的误分类样本，尝试进行修正。

   - 示例：**数据编辑方法（data-editing）**

     > 构建相对邻域图（relative neighborhood graph），其中每一个节点对应一个训练样本，而连接两个不同标签的节点的边被称为切边（cut edge）。
     >
     > 然后，测量一个切边的权重统计量，直觉上，如果一个示例连接了太多的切边，则该示例是可疑的。可疑的示例要么被删除，要么被重新标记。
     >
     > 但是，这种方法通常依赖于咨询邻域信息；由于当数据很稀疏时，邻域识别将变得更不可靠，因此，在高维特征空间中该方法的可靠性将变弱。
     >
     > ![graph4](/Blog-Share/img/1903/04/iyin/graph4.png)

   - 不准确监督的场景：**众包模式**

     > 对于机器学习来说，用众包模式为训练数据收集标是一种经济的方式。未标记的数据被外包给大量的工人来标记。
     >
     > 在著名的众包系统 Amazon Mechanical Turk 上，用户可以提交一项任务，例如将图片标注为树或非树，然后职工完成工作以获取少量报酬。通常这些工人来自世界各地，每个人都可以执行多个任务。这些职工通常互相独立，报酬不高，并通过自己的判断标记数据。这些职工的标记质量参差不齐，但标记质量信息对于用户来说是不可见的，因为工人的身份是保密的。在这些职工中可能存在垃圾制造者，几乎用随机的标签来标记数据（例如，用机器替代人类赚取报酬），或反抗者，故意给出错误的标签。此外，某些任务可能对一些人来说太难而无法完成。使用众包返回的不准确监督信息来保证学习性能是非常困难的。
     > 解决方式：

     - 多数人投票，可作为基线标准。

     - 如果预期可以对工人质量和任务难度建模，为不同的工人在不同任务上设置权重，可获得更好的效果。

       尝试构建概率模型，使用 EM 评估，也有人事又极大极小熵，概率模型可移除垃圾制造者。

     - Dawid-Skene 模型

       简化问题，假设不同任务的潜在成本是相同的，再来考虑实现有效众包学习的最小代价。

     - 设计有效的众包协议

       提供不确定选项、double or nothing 的激励兼容机制使工人提供基于自己信息的标注，避免垃圾制造者出现。

6. #### 补充

   - 松弛向量

     一般地，对于不同形式的线性规划模型，可以采用一些方法将其化为标准型。其中，当约束条件为“≤”（“≥”）类型的线性规划问题，可在不等式左边加上（或者减去）一个非负的新变量，即可化为等式。这个新增的非负变量称为松弛变量（或剩余变量），也可统称为松弛变量。

     给线性规划加入松弛变量也就是为硬性的阈值加入一定的容错性。

   - 其他类型的弱监督

     主要通过强化学习解决的延时监督等

   - 更为复杂的多类别或回归学习

     以不完全监督为例，除了标注/非标注示例之外，多标签任务可能遇到部分标注示例。即使只考虑标注/未标注数据，其设计选项也比单标签设置多，对于主动学习而言，给出非标注示例，在多标签任务中可以要求给出该示例的所有标签、特定标签或一对标签的相关性排序。
