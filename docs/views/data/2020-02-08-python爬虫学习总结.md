---
layout: post
title: "python爬虫学习总结"
date: 2020-02-08
author: "宋佳"
header-img: http://imgsrc.baidu.com/forum/pic/item/9a827d1ed21b0ef455db90dbd1c451da80cb3ec7.jpg
categories:
    - 数据分析部
tags:
    - python
    - 爬虫
---

# Requests 

## 安装 

1. **安装** 

    管理员权限打开cmd，输入 

    ```
    pip install requests
    ```

    如果总是出现time out的报错，切换成国内镜像源，如清华、百度和阿里云的进行安装。

2. **导入**

    ```python
    import requests
    ```

2. **测试**

    尝试抓取百度的网页
    ``` python
    import requests
    r = requests.get("http://www.baidu.com")
    print(r.status_code)
    #若返回200，则访问正常。
    print(r.encoding) 
    #encoding是从HTTP header中猜测出来的编码方式。
    print(r.apparent_encoding)
    #apparent_encoding是从内容中分析出来的编码方式
    r.encoding = 'utf-8'
    #为了防止出现乱码，将encoding=apparent_encoding
    print(r.text)
    #抓取出了百度网页的内容，为html格式
    ```
 ## Requests库的七个主要方法与HTTP协议对资源的操作的关系

 1. **requests.requests()**

    构造一个请求，是支撑其他方法的基础方法

 2. **requests.get()**

    获取HTML网页的主要信息，对应HTTP对资源操作中的GET    

    返回一个Response对象

 3. **requests.head()**

    获取HTML网页头信息的方法，对应于HTTP操作的HEAD

 4. **requests.post()**

    对网页提交POST请求的方法，对应于HTTP的POST
    HTTP协议对资源的操作中的POST为请求向URL位置的资源后附加新的数据

 5. **requests.put()**

    向网页提交PUT请求的方法，对应于HTTP的PUT   
    HTTP中的PUT为请求向URL位置存储一个资源，覆盖原URL位置的资源

 6. **requests.patch()**

    向网页提交局部修改的请求，对应于HTTP的PATCH
    PATCH为请求局部更新URL位置的资源，即改变该处资源的部分内容

 7. **requests.delete()**

    向HTML网页提交删除请求，对应于HTTP的DELETE

## Response对象

当使用requests.get(url)时，会返回一个Response对象

```python
import requests
r = requests.get("http://www.baidu.com")
type(r)
#返回<class 'requests.models.Response'>
```

Response对象有以下几种属性

|属性|说明|
|---|---|
|r.status_code|200表示成功访问|
|r.text|url对应的页面内容|
|r.encoding|从HTTP header中猜测的相应内容编码方式|
|r.apparent_encoding|从内容中分析出来的编码方式|
|r.contente|HTTP相应内容的二进制形式|

requests库中有个方法用于判断是否访问成功

```python
r.raise_for_status()
#如果不是两百，则失败
```

## 一个爬取网页的通用框架

```python
import requests

def getHTMLText(url):
    try:
        r = requests.get(url, timeout = 30)
        r.raise_for_status() #如果状态不是200，引发HTTPError异常
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return "产生异常"

if __name__ == '__main__'：
    url = "http://www.baidu.com"
    print(getHTMLText(url))
```

# BeautifulSoup

## 安装

1. **安装**

    ```
    pip install bs4
    ```

2. **导入**

    ```python
    from bs4 import BeautifulSoup
    ```

## 基于bs4库的HTML内容查找方法

```python
from bs4 import BeautifulSoup
import requests
r = requests.get("https://python123.io/ws/demo.html")
demo = r.text
soup = BeautifulSoup(demo,"html.parser")
soup.find_all('a')#查找所有a标签
soup.find_all(['a','b'])#查找a标签与b标签，a与b以列表形式作为参数。参数为true时，返回所有标签
for tag in soup.find_all(True):
    print(tag.name)
#显示以b开头的标签
import re
for tag in soup.find_all(re.comile('b')):
    print(tag.name)#会打印出b和body

#打印出带有某个字符串的标签，如带有course的p标签
soup.find_all('p','course')

#返回属性中id的值是link1的元素
soup.find_all(id='link1')

#以link开头的标签信息
import re
soup.find_all(id=re.compile('link'))

#recursive
soup.find_all('a')
soup.find_all('a',recursive=False)#改成False后，不对soup的子孙节点查询是否有a标签

#string
soup.find_all(string = "Basic Python")#只能检索出['Basic Python']
import re
soup.find_all(string = re.compile("python"))#把含有python的检索出来
```
## prettify方法

用prettify方法可以使HTML页面更友好地显示，标签不会挤在一团

```python
from bs4 import BeautifulSoup
import requests
r = requests.get("https://python123.io/ws/demo.html")
demo = r.text
soup = BeautifulSoup(demo,"html.parser")
print(soup.prettify())
print(soup.a.prettify())#也可以单独提某个标签出来用这个方法，打印出来很清晰
```

## HTML的内容遍历

### 下行遍历

用到的BeautifulSoup对象的属性有

|属性|说明|
|---|---|
|contents|将所有儿子节点存入列表|
|children|子节点的迭代类型，用于循环遍历儿子节点|
|descendants|子孙节点的迭代类型|

标签树的下行遍历

```python
import requests
from bs4 import BeautifulSoup

r = requests.get("https://python123.io/ws/demo.html")
demo = r.text
soup = BeautifulSoup(demo,"html.parser")
print(soup.head)
print(soup.head.contents)
print(soup.body.contents)#子节点的列表
print(len(soup.body.contents))#可用len函数获取body的儿子节点的数量

#遍历儿子节点
for child in soup.body.children:
    print(child)
#遍历子孙节点
for child in soup.body.descendants:
    print(child)
```

### 上行遍历

上行遍历用到的属性

|属性|说明|
|---|---|
|parent|节点的父亲标签|
|parents|节点先辈标签的迭代类型|

标签树的上行遍历

```python
import requests
from bs4 import BeautifulSoup

r = requests.get("https://python123.io/ws/demo.html")
demo = r.text
soup = BeautifulSoup(demo,"html.parser")
print(soup.title.parent)
print(soup.html.parent)#html的父亲就是自己
print(soup.parent)#soup的父亲为空

soup = BeautifulSoup(demo,"html.parser")
for parent in soup.a.parents:
    if parent is None:
        print(parent)
    else:
        print(parent.name)
#遍历时也会遍历到soup自己，此时parent为none，就不能打印
```

### 平行遍历

所有的平行遍历必须发生在同一个父节点下的各节点间
平行遍历时用到的属性

|属性|说明|
|---|---|
|next_sibling|返回 按照HTML文本顺序 的下一给平行节点标签|
|previous_sibling|返回 按照HTML文本顺序的 上一个平行节点标签|
|next_siblings|迭代类型。返回 按照HTML文本顺序的 后续所有平行节点标签|
|previous_siblings|迭代类型。返回 按照HTML文本顺序的 前续所有平行节点标签|

标签树的平行遍历

```python
import requests
from bs4 import BeautifulSoup

r = requests.get("https://python123.io/ws/demo.html")
demo = r.text
soup = BeautifulSoup(demo,"html.parser")

print(soup.a.next_sibling)
print(soup.a.next_sibling.next_sibling)#a的下一个标签的下一个标签
print(soup.a.previous_sibling)#a标签的前一个平行节点

#遍历后续节点
for sibling in soup.a.next_siblings:
    print(sibling)
#遍历前续节点
for sibling in soup.a.previous_siblings:
    print(sibling)
```

# 正则表达式与re库

## 正则表达式

**正则表达式的常用操作符**

|操作符|说明|实例|
|---|---|---|
|{m}|扩展前一个字符m次|ab{2}c表示abbc|
|{m，n}|扩展前一个字符m至n次，含n|ab{1，2}c表示abc、abbc|
|^|匹配字符串开头|^abc表示abc且在一个字符串的开头|
|$|匹配字符串结尾|abc$表示且在一个字符串的结尾|
|()|分组标记，内部只能用\|操作符|(abc)表示abc，(abc\|def)表示abc、def|
|\d|数字，等价于[0-9]| |
|\w|单词字符，等价于[A-Za-z0-9_]| |

**经典正则表达式实例**

|表达式|含义|
|---|---|
|\^[A-Za-z]|由26个字母组成的字符串|
|\^[A-Za-z0-9]+$|由26个字母和数字组成的字符串|
|^-?\d+$|整数形式的字符串|
|\^[0-9]*[1-9][0-9]*$|正整数形式的字符串|
|[1-9]\d{5}|中国境内邮政编码，六位|
|[\u4e00-\u9fa5]|匹配中文字符|
|\d{3}-\d{8}\|\d{4}-\d{7}|国内电话号码，010-68913536|

## Re库

### 主要函数

1. **re.rearch()**

    在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象
    ```python
    import re
    match = re.search(r'[1-9]\d{5}','BIT 100081')#search函数返回的是match对象
    if match:
        print(match.group(0))
    #返回100081
    print(type(match))#检查match的类型
    ```

2. **re.match()**

    从一个字符串的开始位置起匹配正则表达式，返回match对象
    ```python
    import re
    match = re.match(r'[1-9]\d{5}','BIT 100081')
    if match:
        print(match.group(0))
    #此时match是空，从头开始匹配不了，所以要加if语句判断match是不是空的，把这种情况排除。
    #返回100081
    ```

3. **re.findall()**

    搜索字符串，以列表类型返回全部能匹配的子串
    ```python
    import re
    ls = re.findall(r'[1-9]\d{5}','BIT100081 TSU100084')
    print(ls)
    #返回['100081', '100084']
    ```

4. **re.split()**

    将一个字符串按照正则表达式结果进行分割，返回列表类型
    ```python
    import re
    print(re.split(r'[1-9]\d{5}','BIT100081 TSU100084'))
    #返回['BIT', ' TSU', '']
    print(re.split(r'[1-9]\d{5}','BIT100081 TSU100084', maxsplit=1))
    #maxsplit为最大分割数，剩余部分作为最后一个元素输出
    #返回['BIT', ' TSU100084']
    ```

5. **re.finditer()**

    搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象
    ```python
    import re
    for m in re.finditer(r'[1-9]\d{5}','BIT100081 TSU100084'):
        if m:
            print(m.group(0))
    #迭代地获得结果，对每个结果单独计数
    #返回100081
        #100084
    ```
6. **re.sub()**

    在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串
    ```python
    import re
    print(re.sub(r'[1-9]\d{5}', ':zipcode', 'BIT100081 TSU100084'))
    #返回BIT:zipcode TSU:zipcode
    ```

### Re库的一种等价用法

```python
#函数式用法：一次性操作
rst = re.search(r'[1-9]\d{5}','BIT 100081')
```

等价于

```python
#面向对象用法：编译后的多次操作
pat = re.compile(r'[1-9]\d{5}')
rst = pat.search('BIT 100081')
```

**compile函数：**

```python
regex = re.compile(pattern,flags=0)
```

将正则表达式的字符串形式编译成正则表达式对象
+ pattern：正则表达式的字符串或原生字符串表示
+ flags：正则表达式使用时的控制标记

### Match对象

1. **属性**

    |属性|说明|
    |---|---|
    |.string|待匹配的文本|
    |.re|匹配时使用的pattern对象（正则表达式）|
    |.pos|正则表达式搜索文本的开始位置|
    |.endpos|正则表达式搜索文本的结束位置|

2. **方法**

    |方法|说明|
    |---|---|
    |.group(0)|获得匹配后的字符串|
    |.start()|匹配字符串在原始字符串的开始位置|
    |.end()|匹配字符串在原始字符串的结束位置|
    |.span()|返回(.start(),.end())|

3. **实例**

    ```python
    import re
    m = re.search(r'[1-9]\d{5}','BIT100081 TSU100084')
    print(m.string)#输出BIT100081 TSU100084

    print(m.re)
    #看看程序输出的正则表达式，是带compile的
    #输出re.compile('[1-9]\\d{5}')

    print(m.pos)
    #搜索字符串的开始位置
    #输出0

    print(m.endpos)
    #搜索字符串的结束位置
    #输出19

    print(m.group(0))
    #看看匹配结果，会储存在group0中，match对象只返回第一次匹配的结果
    #输出100081

    print(m.start())
    #匹配结果的起始位置
    #输出3

    print(m.end())
    #匹配结果的终止位置
    #输出9

    print(m.span())
    #匹配结果的起始位置与终止位置的关系
    #输出（3，9）
    ```

### Match对象的贪婪匹配与最小匹配

```python
import re
match = re.rearch(r'PY.*N', 'PYANBNCNDN')
#PY.*N表示以PY开头，以N结尾，中间任意的字符串
print(match.group(0))
#贪婪匹配：re库默认采用贪婪匹配，即输出匹配最长的子串.所以不加任何东西会输出PYANBNCNDN

match = re.rearch(r'PY.*?N', 'PYANBNCNDN')
#最小匹配：在*后面加一个？，会输出最短的子串，即PYAN
```

最小匹配操作符

|操作符|说明|
|---|---|
|*?|前一次字符0次或无限次扩展，最小匹配|
|+?|前一个字符1次或无限次扩展，最小匹配|
|??|前一个字符0次或1次扩展，最小匹配|
|{m,n}?|扩展前一个字符m至n次（含n），最小匹配|

# Scrapy

## Scrapy的安装

+ **参考：**

    [Python爬虫Scrapy框架学习第1课 Win10系统下scrapy安装和环境搭建]( https://blog.csdn.net/weixin_42057995/article/details/89431533
    )  

1. 安装wheel

    打开cmd，输入```pip3 install wheel```

2. 安装lxml

+ 打开cmd，输入```pip3 install lxml```.
这种方法速度极慢，经常出现time out，推荐下一种

+ 下载文件
①打开cmd，输入python，可得python的版本和位数.
(虽然我的电脑是64位，但是python显示on win32，所以我下载win32的whl文件才没有报错)
②打开[lxml的下载地址](https://pypi.org/project/lxml/#files)下载对应的文件
③打开cmd，输入pip3 install 文件所在全路径\lxml-4.3.3-cp37-cp37m-win32.whl

3. 安装PyOpenssl

    像安装lxml一样找到对应版本下载，再在cmd中输入路径/xxx.whl
    [PyOpenssl的下载地址](https://pypi.org/project/pyOpenSSL/#files)

4. 安装Twisted

    同上
    [Twisted的下载地址](https://www.lfd.uci.edu/~gohlke/pythonlibs/)

5. 安装Pywin32

    找到合适的版本安装，是exe文件，一路next下去就行
    [Pywin32的下载地址](https://github.com/mhammond/pywin32/releases)

6. 安装Scrapy

+ 输入```pip3 install scrapy```
速度极慢，经常出错，建议不要尝试直接使用国内镜像源
+ 输入```pip install scrapy -i http://mirrors.aliyun.com/pypi/simple/```

## Scrapy框架简介

Scrapy框架是5+2结构，由五个主要模块与两个中间键组成

### 五个主要模块

1. Scheduler

    + 对所有爬取请求进行调度管理
    + 不需要用户修改

2. Engine（核心）

    + 不需要用户修改
    + 控制所有模块之间的数据流
    + 根据条件触发事件

3. Downloader

    + 根据请求下载网页
    + 不需要用户修改

4. Spider

    + 解析Downloader返回的响应（Response）
    + 产生爬取页（scraped item）
    + 产生额外的爬取请求（Request）
    + 需要用户编写配置代码

5. Item-Piplines

    + 以流水线方式处理Spider产生的爬取项
    + 由一组操作顺序组成，类似流水线，每个操作是一个Item Pipeline类型
    + 可能操作包括：清理、检验和查重爬取项中的HTML数据、将数据存储到数据库
    + 需要用户编写配置代码

### 两个中间键

1. Spider-middleware

    + 目的：对请求和爬取项在处理
    + 功能：修改，丢弃，新增请求或爬取项 
    + 用户可以编写配置代码

2. Downloader-middleware 

    + 目的：实施Engine、Scheduler和Downloader之间进行用户可配置的控制
    + 功能：修改、丢弃、新增请求或响应
    + 用户可以编写配置代码

### 三条数据流的路径

1. 路径1

    + Engine从Spider获得了爬取用户的请求request，可以理解为一个url
    + Engine又将请求转发给了Scheduler

2. 路径2

    + Engine从Scheduler获得下一个要爬取的网路请求，是真实的要去网络上爬取的请求，通过中间键发给Downloader
    + Downloader拿到请求后，连接互联网并爬取网页。将爬取到的内容封装成一个Response对象，通过中间键、Engine发给Spiders

3. 路径3

    + Spider处理从Downloader获得的响应，也就是爬取的内容。产生了两个数据类型：爬取项（item）与新的爬取请求（爬取后发现了新的需要爬取的链接），将它们发送给Engine
    + Engine将items发送给Item-Piplines，将request发给Scheduler进行调度，为后期的数据处理与再次启动爬取请求提供了数据来源。

## Scrapy的使用步骤

1. 创建一个工程和Spider模板

2. 编写Spider编写Spider

3. 编写Item Pipeline编写Item Pipeline

4. 优化配置策略优化配置策略

## Scrapy的常用命令

|命令|说明|格式|
|---|---|---|
|startproject|创建一个新工程|scrapy startproject<name>[dir]|
|genspider|创建一个爬虫|scrapy genspider [options]< name>< domain>|
|settings|获得爬虫配置信息|scrapy settings[options]|
|crawl|运行一个爬虫|scrapy crawl<spider>|
|list|列出工程中所有爬虫|scrapy list|
|shell|启动URL调试命令行|scrapy shell[url]|

## Scrapy与requests的对比

|requests|Scrapy|
|---|---|
|页面级爬虫|网站级爬虫|
|功能库|框架|
|并发性考虑不足，性能较差|并发性好，性能较高|
|重点为页面下载|重点为爬虫结构|
|定制灵活|一般定制灵活，深度定制困难|
|简单|困难|
