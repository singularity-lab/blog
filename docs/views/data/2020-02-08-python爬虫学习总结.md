---
layout: post
title: python爬虫学习总结
date: 2020-02-08
author: 宋佳
header-img: http://imgsrc.baidu.com/forum/pic/item/9a827d1ed21b0ef455db90dbd1c451da80cb3ec7.jpg
categories:
  - 数据分析部
tags:
  - python
  - 爬虫
---

# Requests

## 安装

1. **安装**

   管理员权限打开 cmd，输入

   ```
   pip install requests
   ```

   如果总是出现 time out 的报错，切换成国内镜像源，如清华、百度和阿里云的进行安装。

2. **导入**

   ```python
   import requests
   ```

3. **测试**

   尝试抓取百度的网页

   ```python
   import requests
   r = requests.get("http://www.baidu.com")
   print(r.status_code)
   #若返回200，则访问正常。
   print(r.encoding)
   #encoding是从HTTP header中猜测出来的编码方式。
   print(r.apparent_encoding)
   #apparent_encoding是从内容中分析出来的编码方式
   r.encoding = 'utf-8'
   #为了防止出现乱码，将encoding=apparent_encoding
   print(r.text)
   #抓取出了百度网页的内容，为html格式
   ```

## Requests 库的七个主要方法与 HTTP 协议对资源的操作的关系

1.  **requests.requests()**

    构造一个请求，是支撑其他方法的基础方法

2.  **requests.get()**

    获取 HTML 网页的主要信息，对应 HTTP 对资源操作中的 GET

    返回一个 Response 对象

3.  **requests.head()**

    获取 HTML 网页头信息的方法，对应于 HTTP 操作的 HEAD

4.  **requests.post()**

    对网页提交 POST 请求的方法，对应于 HTTP 的 POST
    HTTP 协议对资源的操作中的 POST 为请求向 URL 位置的资源后附加新的数据

5.  **requests.put()**

    向网页提交 PUT 请求的方法，对应于 HTTP 的 PUT  
    HTTP 中的 PUT 为请求向 URL 位置存储一个资源，覆盖原 URL 位置的资源

6.  **requests.patch()**

    向网页提交局部修改的请求，对应于 HTTP 的 PATCH
    PATCH 为请求局部更新 URL 位置的资源，即改变该处资源的部分内容

7.  **requests.delete()**

    向 HTML 网页提交删除请求，对应于 HTTP 的 DELETE

## Response 对象

当使用 requests.get(url)时，会返回一个 Response 对象

```python
import requests
r = requests.get("http://www.baidu.com")
type(r)
#返回<class 'requests.models.Response'>
```

Response 对象有以下几种属性

| 属性                | 说明                                    |
| ------------------- | --------------------------------------- |
| r.status_code       | 200 表示成功访问                        |
| r.text              | url 对应的页面内容                      |
| r.encoding          | 从 HTTP header 中猜测的相应内容编码方式 |
| r.apparent_encoding | 从内容中分析出来的编码方式              |
| r.contente          | HTTP 相应内容的二进制形式               |

requests 库中有个方法用于判断是否访问成功

```python
r.raise_for_status()
#如果不是两百，则失败
```

## 一个爬取网页的通用框架

```python
import requests

def getHTMLText(url):
    try:
        r = requests.get(url, timeout = 30)
        r.raise_for_status() #如果状态不是200，引发HTTPError异常
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return "产生异常"

if __name__ == '__main__'：
    url = "http://www.baidu.com"
    print(getHTMLText(url))
```

# BeautifulSoup

## 安装

1. **安装**

   ```
   pip install bs4
   ```

2. **导入**

   ```python
   from bs4 import BeautifulSoup
   ```

## 基于 bs4 库的 HTML 内容查找方法

```python
from bs4 import BeautifulSoup
import requests
r = requests.get("https://python123.io/ws/demo.html")
demo = r.text
soup = BeautifulSoup(demo,"html.parser")
soup.find_all('a')#查找所有a标签
soup.find_all(['a','b'])#查找a标签与b标签，a与b以列表形式作为参数。参数为true时，返回所有标签
for tag in soup.find_all(True):
    print(tag.name)
#显示以b开头的标签
import re
for tag in soup.find_all(re.comile('b')):
    print(tag.name)#会打印出b和body

#打印出带有某个字符串的标签，如带有course的p标签
soup.find_all('p','course')

#返回属性中id的值是link1的元素
soup.find_all(id='link1')

#以link开头的标签信息
import re
soup.find_all(id=re.compile('link'))

#recursive
soup.find_all('a')
soup.find_all('a',recursive=False)#改成False后，不对soup的子孙节点查询是否有a标签

#string
soup.find_all(string = "Basic Python")#只能检索出['Basic Python']
import re
soup.find_all(string = re.compile("python"))#把含有python的检索出来
```

## prettify 方法

用 prettify 方法可以使 HTML 页面更友好地显示，标签不会挤在一团

```python
from bs4 import BeautifulSoup
import requests
r = requests.get("https://python123.io/ws/demo.html")
demo = r.text
soup = BeautifulSoup(demo,"html.parser")
print(soup.prettify())
print(soup.a.prettify())#也可以单独提某个标签出来用这个方法，打印出来很清晰
```

## HTML 的内容遍历

### 下行遍历

用到的 BeautifulSoup 对象的属性有

| 属性        | 说明                                   |
| ----------- | -------------------------------------- |
| contents    | 将所有儿子节点存入列表                 |
| children    | 子节点的迭代类型，用于循环遍历儿子节点 |
| descendants | 子孙节点的迭代类型                     |

标签树的下行遍历

```python
import requests
from bs4 import BeautifulSoup

r = requests.get("https://python123.io/ws/demo.html")
demo = r.text
soup = BeautifulSoup(demo,"html.parser")
print(soup.head)
print(soup.head.contents)
print(soup.body.contents)#子节点的列表
print(len(soup.body.contents))#可用len函数获取body的儿子节点的数量

#遍历儿子节点
for child in soup.body.children:
    print(child)
#遍历子孙节点
for child in soup.body.descendants:
    print(child)
```

### 上行遍历

上行遍历用到的属性

| 属性    | 说明                   |
| ------- | ---------------------- |
| parent  | 节点的父亲标签         |
| parents | 节点先辈标签的迭代类型 |

标签树的上行遍历

```python
import requests
from bs4 import BeautifulSoup

r = requests.get("https://python123.io/ws/demo.html")
demo = r.text
soup = BeautifulSoup(demo,"html.parser")
print(soup.title.parent)
print(soup.html.parent)#html的父亲就是自己
print(soup.parent)#soup的父亲为空

soup = BeautifulSoup(demo,"html.parser")
for parent in soup.a.parents:
    if parent is None:
        print(parent)
    else:
        print(parent.name)
#遍历时也会遍历到soup自己，此时parent为none，就不能打印
```

### 平行遍历

所有的平行遍历必须发生在同一个父节点下的各节点间
平行遍历时用到的属性

| 属性              | 说明                                                     |
| ----------------- | -------------------------------------------------------- |
| next_sibling      | 返回 按照 HTML 文本顺序 的下一给平行节点标签             |
| previous_sibling  | 返回 按照 HTML 文本顺序的 上一个平行节点标签             |
| next_siblings     | 迭代类型。返回 按照 HTML 文本顺序的 后续所有平行节点标签 |
| previous_siblings | 迭代类型。返回 按照 HTML 文本顺序的 前续所有平行节点标签 |

标签树的平行遍历

```python
import requests
from bs4 import BeautifulSoup

r = requests.get("https://python123.io/ws/demo.html")
demo = r.text
soup = BeautifulSoup(demo,"html.parser")

print(soup.a.next_sibling)
print(soup.a.next_sibling.next_sibling)#a的下一个标签的下一个标签
print(soup.a.previous_sibling)#a标签的前一个平行节点

#遍历后续节点
for sibling in soup.a.next_siblings:
    print(sibling)
#遍历前续节点
for sibling in soup.a.previous_siblings:
    print(sibling)
```

# 正则表达式与 re 库

## 正则表达式

**正则表达式的常用操作符**

| 操作符 | 说明                           | 实例                                   |
| ------ | ------------------------------ | -------------------------------------- |
| {m}    | 扩展前一个字符 m 次            | ab{2}c 表示 abbc                       |
| {m，n} | 扩展前一个字符 m 至 n 次，含 n | ab{1，2}c 表示 abc、abbc               |
| ^      | 匹配字符串开头                 | ^abc 表示 abc 且在一个字符串的开头     |
| \$     | 匹配字符串结尾                 | abc\$表示且在一个字符串的结尾          |
| ()     | 分组标记，内部只能用\|操作符   | (abc)表示 abc，(abc\|def)表示 abc、def |
| \d     | 数字，等价于[0-9]              |                                        |
| \w     | 单词字符，等价于[A-Za-z0-9_]   |                                        |

**经典正则表达式实例**

| 表达式                   | 含义                           |
| ------------------------ | ------------------------------ |
| \^[A-Za-z]               | 由 26 个字母组成的字符串       |
| \^[A-Za-z0-9]+\$         | 由 26 个字母和数字组成的字符串 |
| ^-?\d+\$                 | 整数形式的字符串               |
| \^[0-9]_[1-9][0-9]_\$    | 正整数形式的字符串             |
| [1-9]\d{5}               | 中国境内邮政编码，六位         |
| [\u4e00-\u9fa5]          | 匹配中文字符                   |
| \d{3}-\d{8}\|\d{4}-\d{7} | 国内电话号码，010-68913536     |

## Re 库

### 主要函数

1. **re.rearch()**

   在一个字符串中搜索匹配正则表达式的第一个位置，返回 match 对象

   ```python
   import re
   match = re.search(r'[1-9]\d{5}','BIT 100081')#search函数返回的是match对象
   if match:
       print(match.group(0))
   #返回100081
   print(type(match))#检查match的类型
   ```

2. **re.match()**

   从一个字符串的开始位置起匹配正则表达式，返回 match 对象

   ```python
   import re
   match = re.match(r'[1-9]\d{5}','BIT 100081')
   if match:
       print(match.group(0))
   #此时match是空，从头开始匹配不了，所以要加if语句判断match是不是空的，把这种情况排除。
   #返回100081
   ```

3. **re.findall()**

   搜索字符串，以列表类型返回全部能匹配的子串

   ```python
   import re
   ls = re.findall(r'[1-9]\d{5}','BIT100081 TSU100084')
   print(ls)
   #返回['100081', '100084']
   ```

4. **re.split()**

   将一个字符串按照正则表达式结果进行分割，返回列表类型

   ```python
   import re
   print(re.split(r'[1-9]\d{5}','BIT100081 TSU100084'))
   #返回['BIT', ' TSU', '']
   print(re.split(r'[1-9]\d{5}','BIT100081 TSU100084', maxsplit=1))
   #maxsplit为最大分割数，剩余部分作为最后一个元素输出
   #返回['BIT', ' TSU100084']
   ```

5. **re.finditer()**

   搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是 match 对象

   ```python
   import re
   for m in re.finditer(r'[1-9]\d{5}','BIT100081 TSU100084'):
       if m:
           print(m.group(0))
   #迭代地获得结果，对每个结果单独计数
   #返回100081
       #100084
   ```

6. **re.sub()**

   在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串

   ```python
   import re
   print(re.sub(r'[1-9]\d{5}', ':zipcode', 'BIT100081 TSU100084'))
   #返回BIT:zipcode TSU:zipcode
   ```

### Re 库的一种等价用法

```python
#函数式用法：一次性操作
rst = re.search(r'[1-9]\d{5}','BIT 100081')
```

等价于

```python
#面向对象用法：编译后的多次操作
pat = re.compile(r'[1-9]\d{5}')
rst = pat.search('BIT 100081')
```

**compile 函数：**

```python
regex = re.compile(pattern,flags=0)
```

将正则表达式的字符串形式编译成正则表达式对象

- pattern：正则表达式的字符串或原生字符串表示
- flags：正则表达式使用时的控制标记

### Match 对象

1. **属性**

   | 属性    | 说明                                    |
   | ------- | --------------------------------------- |
   | .string | 待匹配的文本                            |
   | .re     | 匹配时使用的 pattern 对象（正则表达式） |
   | .pos    | 正则表达式搜索文本的开始位置            |
   | .endpos | 正则表达式搜索文本的结束位置            |

2. **方法**

   | 方法      | 说明                             |
   | --------- | -------------------------------- |
   | .group(0) | 获得匹配后的字符串               |
   | .start()  | 匹配字符串在原始字符串的开始位置 |
   | .end()    | 匹配字符串在原始字符串的结束位置 |
   | .span()   | 返回(.start(),.end())            |

3. **实例**

   ```python
   import re
   m = re.search(r'[1-9]\d{5}','BIT100081 TSU100084')
   print(m.string)#输出BIT100081 TSU100084

   print(m.re)
   #看看程序输出的正则表达式，是带compile的
   #输出re.compile('[1-9]\\d{5}')

   print(m.pos)
   #搜索字符串的开始位置
   #输出0

   print(m.endpos)
   #搜索字符串的结束位置
   #输出19

   print(m.group(0))
   #看看匹配结果，会储存在group0中，match对象只返回第一次匹配的结果
   #输出100081

   print(m.start())
   #匹配结果的起始位置
   #输出3

   print(m.end())
   #匹配结果的终止位置
   #输出9

   print(m.span())
   #匹配结果的起始位置与终止位置的关系
   #输出（3，9）
   ```

### Match 对象的贪婪匹配与最小匹配

```python
import re
match = re.rearch(r'PY.*N', 'PYANBNCNDN')
#PY.*N表示以PY开头，以N结尾，中间任意的字符串
print(match.group(0))
#贪婪匹配：re库默认采用贪婪匹配，即输出匹配最长的子串.所以不加任何东西会输出PYANBNCNDN

match = re.rearch(r'PY.*?N', 'PYANBNCNDN')
#最小匹配：在*后面加一个？，会输出最短的子串，即PYAN
```

最小匹配操作符

| 操作符 | 说明                                       |
| ------ | ------------------------------------------ |
| \*?    | 前一次字符 0 次或无限次扩展，最小匹配      |
| +?     | 前一个字符 1 次或无限次扩展，最小匹配      |
| ??     | 前一个字符 0 次或 1 次扩展，最小匹配       |
| {m,n}? | 扩展前一个字符 m 至 n 次（含 n），最小匹配 |

# Scrapy

## Scrapy 的安装

- **参考：**

  [Python 爬虫 Scrapy 框架学习第 1 课 Win10 系统下 scrapy 安装和环境搭建](https://blog.csdn.net/weixin_42057995/article/details/89431533)

1. 安装 wheel

   打开 cmd，输入`pip3 install wheel`

2. 安装 lxml

- 打开 cmd，输入`pip3 install lxml`.
  这种方法速度极慢，经常出现 time out，推荐下一种

- 下载文件
  ① 打开 cmd，输入 python，可得 python 的版本和位数.
  (虽然我的电脑是 64 位，但是 python 显示 on win32，所以我下载 win32 的 whl 文件才没有报错)
  ② 打开[lxml 的下载地址](https://pypi.org/project/lxml/#files)下载对应的文件
  ③ 打开 cmd，输入 pip3 install 文件所在全路径\lxml-4.3.3-cp37-cp37m-win32.whl

3. 安装 PyOpenssl

   像安装 lxml 一样找到对应版本下载，再在 cmd 中输入路径/xxx.whl
   [PyOpenssl 的下载地址](https://pypi.org/project/pyOpenSSL/#files)

4. 安装 Twisted

   同上
   [Twisted 的下载地址](https://www.lfd.uci.edu/~gohlke/pythonlibs/)

5. 安装 Pywin32

   找到合适的版本安装，是 exe 文件，一路 next 下去就行
   [Pywin32 的下载地址](https://github.com/mhammond/pywin32/releases)

6. 安装 Scrapy

- 输入`pip3 install scrapy`
  速度极慢，经常出错，建议不要尝试直接使用国内镜像源
- 输入`pip install scrapy -i http://mirrors.aliyun.com/pypi/simple/`

## Scrapy 框架简介

Scrapy 框架是 5+2 结构，由五个主要模块与两个中间键组成

### 五个主要模块

1. Scheduler

   - 对所有爬取请求进行调度管理
   - 不需要用户修改

2. Engine（核心）

   - 不需要用户修改
   - 控制所有模块之间的数据流
   - 根据条件触发事件

3. Downloader

   - 根据请求下载网页
   - 不需要用户修改

4. Spider

   - 解析 Downloader 返回的响应（Response）
   - 产生爬取页（scraped item）
   - 产生额外的爬取请求（Request）
   - 需要用户编写配置代码

5. Item-Piplines

   - 以流水线方式处理 Spider 产生的爬取项
   - 由一组操作顺序组成，类似流水线，每个操作是一个 Item Pipeline 类型
   - 可能操作包括：清理、检验和查重爬取项中的 HTML 数据、将数据存储到数据库
   - 需要用户编写配置代码

### 两个中间键

1. Spider-middleware

   - 目的：对请求和爬取项在处理
   - 功能：修改，丢弃，新增请求或爬取项
   - 用户可以编写配置代码

2. Downloader-middleware

   - 目的：实施 Engine、Scheduler 和 Downloader 之间进行用户可配置的控制
   - 功能：修改、丢弃、新增请求或响应
   - 用户可以编写配置代码

### 三条数据流的路径

1. 路径 1

   - Engine 从 Spider 获得了爬取用户的请求 request，可以理解为一个 url
   - Engine 又将请求转发给了 Scheduler

2. 路径 2

   - Engine 从 Scheduler 获得下一个要爬取的网路请求，是真实的要去网络上爬取的请求，通过中间键发给 Downloader
   - Downloader 拿到请求后，连接互联网并爬取网页。将爬取到的内容封装成一个 Response 对象，通过中间键、Engine 发给 Spiders

3. 路径 3

   - Spider 处理从 Downloader 获得的响应，也就是爬取的内容。产生了两个数据类型：爬取项（item）与新的爬取请求（爬取后发现了新的需要爬取的链接），将它们发送给 Engine
   - Engine 将 items 发送给 Item-Piplines，将 request 发给 Scheduler 进行调度，为后期的数据处理与再次启动爬取请求提供了数据来源。

## Scrapy 的使用步骤

1. 创建一个工程和 Spider 模板

2. 编写 Spider 编写 Spider

3. 编写 Item Pipeline 编写 Item Pipeline

4. 优化配置策略优化配置策略

## Scrapy 的常用命令

| 命令         | 说明                | 格式                                       |
| ------------ | ------------------- | ------------------------------------------ |
| startproject | 创建一个新工程      | scrapy startproject<name>[dir]             |
| genspider    | 创建一个爬虫        | scrapy genspider [options]< name>< domain> |
| settings     | 获得爬虫配置信息    | scrapy settings[options]                   |
| crawl        | 运行一个爬虫        | scrapy crawl<spider>                       |
| list         | 列出工程中所有爬虫  | scrapy list                                |
| shell        | 启动 URL 调试命令行 | scrapy shell[url]                          |

## Scrapy 与 requests 的对比

| requests                 | Scrapy                     |
| ------------------------ | -------------------------- |
| 页面级爬虫               | 网站级爬虫                 |
| 功能库                   | 框架                       |
| 并发性考虑不足，性能较差 | 并发性好，性能较高         |
| 重点为页面下载           | 重点为爬虫结构             |
| 定制灵活                 | 一般定制灵活，深度定制困难 |
| 简单                     | 困难                       |
